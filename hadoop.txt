Hadoop入门

================================

# 大数据生态系统

1. 存储：hadoop HDFS
2. 计算引擎
   Map/Reduce v1
   Map/Reduce v2(Map/Reduce on Yarn)
   Tez
   Spark
3. Impala Presto Drill直接运行在HDFS上
   pig(脚本方式) hive(SQL语言) 运行在Map/Reduce上
   hive on Tez / SparkSQL
4. 流式计算Storm
5. KV Store
   Cassandra
   MongoDB
   HBase
6. Tensorflow
   Mahout
7. Zookeeper
   Protobuf
8. Sqoop
   Kafka
   Flume
   ...

================================

# 安装JDK和hadoop

1.安装虚拟机，使用CentOS系统，虚拟机网络使用host-only模式
  vim /etc/sysconfig/network
    NETWORKING=yes
    GATEWAY=192.168.56.1
  vim /etc/sysconfig/network-scripts/ifcfg-enp0s3
    TYPE=Ethernet
    IPADDR=192.168.56.10
    NETMASK=255.255.255.0
  修改主机名，主机名不要包含下划线
    hostnamectl set-hostname master
  如果需要虚拟机联网，还需要配置/etc/resolv.conf
  重启网络：service network restart

2.将JDK和hadoop安装包上传到虚拟机/usr/local目录
  安装JDK
    rpm -ivh jdk-8u65-linux-x64.rpm
  安装hadoop
    tar -xvf hadoop-2.7.3.tar.gz
    mv hadoop-2.7.3 hadoop

3.设置环境变量
  新建/etc/profile.d/java.sh，写入如下内容
    export JAVA_HOME=/usr/java/default
    export CLASSPATH=.:${JAVA_HOME}/lib
  *若不设置JAVA_HOME环境变量则要修改/usr/local/hadoop/etc/hadoop/hadoop-env.sh
    export JAVA_HOME=/usr/java/default
  新建/etc/profile.d/hadoop.sh，写入如下内容
    export PATH=${PATH}:/usr/local/hadoop/bin:/usr/local/hadoop/sbin
  关闭防火墙
    systemctl stop firewalld.service
    systemctl disable firewalld.service
  修改/etc/hosts，使得主机名可以互相ping通
    192.168.56.10 master
    192.168.56.11 slave1
    192.168.56.12 slave2
    192.168.56.13 slave3
  重启操作系统后验证hadoop命令是否可用

4.复制以上虚拟机，分别修改IP/主机名信息
  修改C:\Windows\System32\drivers\etc\hosts，添加如下内容
    192.168.56.10 master
    192.168.56.11 slave1
    192.168.56.12 slave2
    192.168.56.13 slave3  
  即可使用主机名访问虚拟机中的服务

================================

# 启动/停止hadoop集群

1.HDFS架构[理解NameNode和DataNode]
  NameNode
  - 存储文件系统元数据（文件目录结构、分块情况、每块位置、权限等）
  - 这部分元数据存储在内存中，因此要求内存比较大
  DataNode
  - 存储数据块（block），默认1block=128M（即使文件小于128M）
  - hadoop适合存储大文件
  - 大于128M的文件会进行分block，默认每个block在不同的DataNode上共存储三份

2.所有虚拟机修改配置/usr/local/hadoop/etc/hadoop/core-site.xml
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://master:9000</value>
  </property>

3.在虚拟机master上启动NameNode
  hdfs namenode -format
  hadoop-daemon.sh start namenode
  此时终端输出默认日志输出路径/usr/local/hadoop/logs/hadoop-root-namenode-master.out
  jps查看后台运行任务中是否有NameNode进程

4.在虚拟机slave*上启动DataNode
  hadoop-daemon.sh start datanode
  此时终端输出默认日志输出路径/usr/local/hadoop/logs/hadoop-root-datanode-slave*.out
  jps查看后台运行任务中是否有DataNode进程

5.以上启动的hadoop集群
  物理架构：master + slave1 + slave2 + slave3
  逻辑架构：namenode + datanode1 + datanode2 + datanode3

6.查看当前集群状态
  命令：hdfs dfsadmin -report
  网页：http://master:50070/

7.停止hadoop集群
  停止NameNode：hadoop-daemon.sh stop namenode
  停止DataNode：hadoop-daemon.sh stop datanode

8.集中管理hadoop集群
  编辑/usr/local/hadoop/etc/hadoop/slaves，将所有slave主机名写入该配置文件，一个主机名一行
  使用命令启动/停止整个集群
  - 启动：start-dfs.sh（需要输入master和所有slave的主机密码）
  - 停止：stop-dfs.sh（需要输入master和所有slave的主机密码）

9.配置master到slave的免密登录
  为master生成ssh-key：ssh-keygen -t rsa
  将master公钥复制到master的.ssh/authorized_keys文件：ssh-copy-id master
  将master的公钥复制到所有slave主机上：
    ssh-copy-id slave1
    ssh-copy-id slave2
    ssh-copy-id slave3
  再次尝试运行start-dfs.sh和stop-dfs.sh就不需要输入密码了

================================

# 访问HDFS

1.操作HDFS的命令[hadoop fs]或[hdfs dfs]
  hadoop fs -ls /
  hadoop fs -put test.txt /
  hadoop fs -text /test.txt
  hdfs dfs -rm -f /test.txt
  hdfs dfs -mkdir /tmp
  hdfs dfs -rm -r -f /tmp

2.修改/usr/local/hadoop/etc/hadoop/hdfs-site.xml中的dfs.replication选项为2
  参考hadoop文档：hadoop-2.7.3/share/doc/hadoop/index.html中的hdfs-default.xml
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
  重启hadoop集群，上传文件apache-hive-2.1.1-bin.tar.gz到hadoop
  查看replication和block存储状态
  - replication应为2，文件被分为两个block，分别存储在不同的slave上
  - 如：block0存储在slave1/slave3，block1存储在slave1/slave2

3.修改/usr/local/hadoop/etc/hadoop/hdfs-site.xml中的心跳检测间隔时间
  dfs.namenode.heartbeat.recheck-interval选项为10000(ms)，该值默认为300000(ms)
  然后停掉一个DataNode，如：slave3，通过网页查看DataNode和文件block的复制情况
  <property>
    <name>dfs.namenode.heartbeat.recheck-interval</name>
    <value>10000</value>
  </property>

4.Java访问HDFS

  HDFS系统会把用到的数据存储在core-site.xml中hadoop.tmp.dir指定的目录，默认位于/tmp/hadoop-${user.name}
  由于/tmp目录下的文件在系统重启时会被删除，所以应该修改目录位置，修改所有结点的core-site.xml
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/var/hadoop</value>
  </property>

  上传一个测试文件到HDFS
  hadoop fs -put test.txt /

  通过Java访问HDFS时将集群看作一个大的系统磁盘即可

  因为windows和linux上的权限系统的区别，测试期间关闭权限检查
  修改NameNode上的/usr/local/hadoop/etc/hadoop/hdfs-site.xml
  <property>
    <name>dfs.permissions.enabled</name>
    <value>false</value>
  </property>
  然后重启NameNode
  hadoop-daemon.sh stop  namenode
  hadoop-daemon.sh start namenode
  也可以通过设置环境变量HADOOP_USER_NAME=root来解决这个问题

  Hadoop权限控制：Kerberos

  读取HDFS上的文件：
  URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());
  URL url = new URL("hdfs://192.168.56.10:9000/test.txt");
  InputStream in = url.openStream();
  IOUtils.copyBytes(in, System.out, 4096, true);

  使用FileSystem类
  Configuration conf = new Configuration();
  conf.set("fs.defaultFS", "hdfs://192.168.56.10:9000");
  conf.set("dfs.replication", "2"); //设置dfs.replication属性
  FileSystem fileSystem = FileSystem.get(conf);

  创建删除目录，判断目录是否存在
  fileSystem.exists(new Path("/hello"));
  fileSystem.mkdirs(new Path("/ydd"));
  fileSystem.delete(new Path("/ydd"), true);
  fileSystem.exists(new Path("/ydd"));

  上传文件到HDFS1
  FSDataOutputStream out = fileSystem.create(new Path("/test.data"), true);
  FileInputStream fis = new FileInputStream("hadoop.txt");
  IOUtils.copyBytes(fis, out, 4096, true);

================================

# Hadoop基础回顾

1.Hadoop组件
  HDFS、Yarn、MapReduce
2.HDFS包括
  NameNode
  DataNode
  SecondaryNameNode
  JournalNode
  HDFS典型应用：网盘
3.其他内容
  安全与权限Kerberos
  SecondaryNameNode -> CheckPointNameNode
  HA实现 -> JournalNode
  Federation -> 超大规模数据中心

================================

# 计算调度系统Yarn

1.分布式计算
  设计原则：移动计算，而不是移动数据

2.计算调度Yarn

  两个组件：ResourceManager(管理Job) + NodeManager(执行计算)
  类似HDFS的NameNode和DataNode
  通常NameNode和ResourceManager分开部署，DataNode和NodeManager就近或部署在一起

3.启动Yarn
  - 配置master和所有slave上的/usr/local/hadoop/etc/hadoop/yarn-site.xml
    <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>master</value>
    </property>
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
    <property>
      <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    注：yarn-site.xml如果是集中启动，只需要在ResourceManager上配置一份即可
    若单独启动，需要每台机器都配置，在网页上可以看到当前机器的配置以及这个配置的来源
  - 执行命令启动：start-yarn.sh
    以上启动命令会检查slaves文件中的主机，在其上启动NodeManager
  - 通过网页查看Yarn状态：http://master:8088/
  - 可以使用yarn-daemon.sh单独启动resourcemanager和nodemanager

4.启动Yarn不以HDFS启动为前提，执行MapReduce也不依赖Yarn

5.在Yarn上执行一个示例任务
  - 配置master上的/usr/local/hadoop/etc/hadoop/mapred-site.xml
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value><!-- 默认是local -->
    </property>
  - 上传一个测试文件到HDFS
    vim input.txt
      hello java
      hello java
      hello c
      hello scala
      hello kotlin
      hello c++
      hello python
      hello java
      hello c#
      hello perl
    hadoop fs -mkdir /input
    hadoop fs -put input.txt /input/
  - 查找hadoop安装目录下的示例程序
    find /usr/local/hadoop/ -name *example*.jar
    /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar
  - 执行MapReduce示例WordCount，统计/input/input.txt中的单词数量
    hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount /input/input.txt /output
  - 查看控制台输出的JobID
  - 通过网页查询Yarn中Application的状态，以及任务执行状态
  - 执行完成后在HDFS的/output目录查看执行结果
    任务执行成功：_SUCCESS
    任务执行结果：part-r-00000
    hadoop fs -text /output/part-r-00000
      c         1
      c#        1
      c++       1
      hello     10
      java      3
      kotlin    1
      perl      1
      python    1
      scala     1

================================

# 计算引擎Map/Reduce

1.WordCount工作过程
  input  : 输入，input.txt的内容
  split  : 拆分，默认按行拆分，形成K-V对（起始位置->数据行）
  map    : 映射，计算split结果中的数据行，形成K-V对（单词->1，可能存在的缩减：单词->n）
  shuffle: 派发，将map的结果分发到各结点进行计算（单词->[1,1,n,1,...]）
  reduce : 缩减，对shuffle过程分发过来的数据进行缩减计算得到结果（单词->数量）
  output : 输出，最终结果

2.Java开发MpaReduce程序

  配置环境变量
  - HADOOP_HOME=hadoop的安装目录(路径不要包含空格或中文)
  - 为方便操作，将%HADOOP_HOME%\bin添加到Path环境变量

  对于windows系统，需要添加windows的库文件
  - 将Windows库文件覆盖到%HADOOP_HOME%\bin
  - 若有问题，可将hadoop.dll复制到C:\Windows\System32目录
  设置系统环境变量HADOOP_USER_NAME=root

  Map代码：src/main/java/com/my/project/WordMapper.java
  Reduce代码：src/main/java/com/my/project/WordReducer.java
  MapReduce测试代码：src/test/java/com/my/project/TestMapReduce.java

  在远程Yarn上执行MapReduce程序时除了使用conf.setXXX设置相关属性
    conf.set("fs.defaultFS", "hdfs://192.168.56.10:9000/");
    conf.set("mapreduce.job.jar", "target/hello-hadoop-0.0.1.jar");
    conf.set("mapreduce.framework.name", "yarn");
    conf.set("yarn.resourcemanager.hostname", "192.168.56.10");
    conf.set("mapreduce.app-submission.cross-platform", "true");
  也可以将hadoop的四个配置文件加到src/main/resources目录
    core-site.xml
    hdfs-site.xml
    mapred-site.xml
    yarn-site.xml

