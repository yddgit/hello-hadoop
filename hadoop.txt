Hadoop入门

================================

# 大数据生态系统

1. 存储：hadoop HDFS
2. 计算引擎
   Map/Reduce v1
   Map/Reduce v2(Map/Reduce on Yarn)
   Tez
   Spark
3. Impala Presto Drill直接运行在HDFS上
   pig(脚本方式) hive(SQL语言) 运行在Map/Reduce上
   hive on Tez / SparkSQL
4. 流式计算Storm
5. KV Store
   Cassandra
   MongoDB
   HBase
6. Tensorflow
   Mahout
7. Zookeeper
   Protobuf
8. Sqoop
   Kafka
   Flume
   ...

================================

# 安装JDK和hadoop

1.安装虚拟机，使用CentOS系统，虚拟机网络使用host-only模式
  vim /etc/sysconfig/network
    NETWORKING=yes
    GATEWAY=192.168.56.1
  vim /etc/sysconfig/network-scripts/ifcfg-enp0s3
    TYPE=Ethernet
    IPADDR=192.168.56.10
    NETMASK=255.255.255.0
  修改主机名，主机名不要包含下划线
    hostnamectl set-hostname master
  如果需要虚拟机联网，还需要配置/etc/resolv.conf
  重启网络：service network restart

2.将JDK和hadoop安装包上传到虚拟机/usr/local目录
  安装JDK
    rpm -ivh jdk-8u65-linux-x64.rpm
  安装hadoop
    tar -xvf hadoop-2.7.3.tar.gz
    mv hadoop-2.7.3 hadoop

3.设置环境变量
  新建/etc/profile.d/java.sh，写入如下内容
    export JAVA_HOME=/usr/java/default
    export CLASSPATH=.:${JAVA_HOME}/lib
  *若不设置JAVA_HOME环境变量则要修改/usr/local/hadoop/etc/hadoop/hadoop-env.sh
    export JAVA_HOME=/usr/java/default
  新建/etc/profile.d/hadoop.sh，写入如下内容
    export PATH=${PATH}:/usr/local/hadoop/bin:/usr/local/hadoop/sbin
  关闭防火墙
    systemctl stop firewalld.service
    systemctl disable firewalld.service
  修改/etc/hosts，使得主机名可以互相ping通
    192.168.56.10 master
    192.168.56.11 slave1
    192.168.56.12 slave2
    192.168.56.13 slave3
  重启操作系统后验证hadoop命令是否可用

4.复制以上虚拟机，分别修改IP/主机名信息
  修改C:\Windows\System32\drivers\etc\hosts，添加如下内容
    192.168.56.10 master
    192.168.56.11 slave1
    192.168.56.12 slave2
    192.168.56.13 slave3  
  即可使用主机名访问虚拟机中的服务

================================

# 启动/停止hadoop集群

1.HDFS架构[理解NameNode和DataNode]
  NameNode
  - 存储文件系统元数据（文件目录结构、分块情况、每块位置、权限等）
  - 这部分元数据存储在内存中，因此要求内存比较大
  DataNode
  - 存储数据块（block），默认1block=128M（即使文件小于128M）
  - hadoop适合存储大文件
  - 大于128M的文件会进行分block，默认每个block在不同的DataNode上共存储三份

2.所有虚拟机修改配置/usr/local/hadoop/etc/hadoop/core-site.xml
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://master:9000</value>
  </property>

3.在虚拟机master上启动NameNode
  hdfs namenode -format
  hadoop-daemon.sh start namenode
  此时终端输出默认日志输出路径/usr/local/hadoop/logs/hadoop-root-namenode-master.out
  jps查看后台运行任务中是否有NameNode进程

4.在虚拟机slave*上启动DataNode
  hadoop-daemon.sh start datanode
  此时终端输出默认日志输出路径/usr/local/hadoop/logs/hadoop-root-datanode-slave*.out
  jps查看后台运行任务中是否有DataNode进程

5.以上启动的hadoop集群
  物理架构：master + slave1 + slave2 + slave3
  逻辑架构：namenode + datanode1 + datanode2 + datanode3

6.查看当前集群状态
  命令：hdfs dfsadmin -report
  网页：http://master:50070/

7.停止hadoop集群
  停止NameNode：hadoop-daemon.sh stop namenode
  停止DataNode：hadoop-daemon.sh stop datanode

8.集中管理hadoop集群
  编辑/usr/local/hadoop/etc/hadoop/slaves，将所有slave主机名写入该配置文件，一个主机名一行
  使用命令启动/停止整个集群
  - 启动：start-dfs.sh（需要输入master和所有slave的主机密码）
  - 停止：stop-dfs.sh（需要输入master和所有slave的主机密码）

9.配置master到slave的免密登录
  为master生成ssh-key：ssh-keygen -t rsa
  将master公钥复制到master的.ssh/authorized_keys文件：ssh-copy-id master
  将master的公钥复制到所有slave主机上：
    ssh-copy-id slave1
    ssh-copy-id slave2
    ssh-copy-id slave3
  再次尝试运行start-dfs.sh和stop-dfs.sh就不需要输入密码了

================================

# 访问HDFS

1.操作HDFS的命令[hadoop fs]或[hdfs dfs]
  hadoop fs -ls /
  hadoop fs -put test.txt /
  hadoop fs -text /test.txt
  hdfs dfs -rm -f /test.txt
  hdfs dfs -mkdir /tmp
  hdfs dfs -rm -r -f /tmp

2.修改/usr/local/hadoop/etc/hadoop/hdfs-site.xml中的dfs.replication选项为2
  参考hadoop文档：hadoop-2.7.3/share/doc/hadoop/index.html中的hdfs-default.xml
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
  重启hadoop集群，上传文件apache-hive-2.1.1-bin.tar.gz到hadoop
  查看replication和block存储状态
  - replication应为2，文件被分为两个block，分别存储在不同的slave上
  - 如：block0存储在slave1/slave3，block1存储在slave1/slave2

3.修改/usr/local/hadoop/etc/hadoop/hdfs-site.xml中的心跳检测间隔时间
  dfs.namenode.heartbeat.recheck-interval选项为10000(ms)，该值默认为300000(ms)
  然后停掉一个DataNode，如：slave3，通过网页查看DataNode和文件block的复制情况
  <property>
    <name>dfs.namenode.heartbeat.recheck-interval</name>
    <value>10000</value>
  </property>

4.Java访问HDFS

  HDFS系统会把用到的数据存储在core-site.xml中hadoop.tmp.dir指定的目录，默认位于/tmp/hadoop-${user.name}
  由于/tmp目录下的文件在系统重启时会被删除，所以应该修改目录位置，修改所有结点的core-site.xml
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/var/hadoop</value>
  </property>

  上传一个测试文件到HDFS
  hadoop fs -put test.txt /

  通过Java访问HDFS时将集群看作一个大的系统磁盘即可

  因为windows和linux上的权限系统的区别，测试期间关闭权限检查
  修改NameNode上的/usr/local/hadoop/etc/hadoop/hdfs-site.xml
  <property>
    <name>dfs.permissions.enabled</name>
    <value>false</value>
  </property>
  然后重启NameNode
  hadoop-daemon.sh stop  namenode
  hadoop-daemon.sh start namenode
  也可以通过设置环境变量HADOOP_USER_NAME=root来解决这个问题

  Hadoop权限控制：Kerberos

  读取HDFS上的文件：
  URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());
  URL url = new URL("hdfs://192.168.56.10:9000/test.txt");
  InputStream in = url.openStream();
  IOUtils.copyBytes(in, System.out, 4096, true);

  使用FileSystem类
  Configuration conf = new Configuration();
  conf.set("fs.defaultFS", "hdfs://192.168.56.10:9000");
  conf.set("dfs.replication", "2"); //设置dfs.replication属性
  FileSystem fileSystem = FileSystem.get(conf);

  创建删除目录，判断目录是否存在
  fileSystem.exists(new Path("/hello"));
  fileSystem.mkdirs(new Path("/ydd"));
  fileSystem.delete(new Path("/ydd"), true);
  fileSystem.exists(new Path("/ydd"));

  上传文件到HDFS1
  FSDataOutputStream out = fileSystem.create(new Path("/test.data"), true);
  FileInputStream fis = new FileInputStream("hadoop.txt");
  IOUtils.copyBytes(fis, out, 4096, true);

================================

# Hadoop基础回顾

1.Hadoop组件
  HDFS、Yarn、MapReduce
2.HDFS包括
  NameNode
  DataNode
  SecondaryNameNode
  JournalNode
  HDFS典型应用：网盘
3.其他内容
  安全与权限Kerberos
  SecondaryNameNode -> CheckPointNameNode
  HA实现 -> JournalNode
  Federation -> 超大规模数据中心

================================

# 计算调度系统Yarn

1.分布式计算
  设计原则：移动计算，而不是移动数据

2.计算调度Yarn

  两个组件：ResourceManager(管理Job) + NodeManager(执行计算)
  类似HDFS的NameNode和DataNode
  通常NameNode和ResourceManager分开部署，DataNode和NodeManager就近或部署在一起

3.启动Yarn
  - 配置master和所有slave上的/usr/local/hadoop/etc/hadoop/yarn-site.xml
    <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>master</value>
    </property>
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
    <property>
      <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    注：yarn-site.xml如果是集中启动，只需要在ResourceManager上配置一份即可
    若单独启动，需要每台机器都配置，在网页上可以看到当前机器的配置以及这个配置的来源
  - 执行命令启动：start-yarn.sh
    以上启动命令会检查slaves文件中的主机，在其上启动NodeManager
  - 通过网页查看Yarn状态：http://master:8088/
  - 可以使用yarn-daemon.sh单独启动resourcemanager和nodemanager

4.启动Yarn不以HDFS启动为前提，执行MapReduce也不依赖Yarn

5.在Yarn上执行一个示例任务
  - 配置master上的/usr/local/hadoop/etc/hadoop/mapred-site.xml
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value><!-- 默认是local -->
    </property>
  - 上传一个测试文件到HDFS
    vim input.txt
      hello java
      hello java
      hello c
      hello scala
      hello kotlin
      hello c++
      hello python
      hello java
      hello c#
      hello perl
    hadoop fs -mkdir /input
    hadoop fs -put input.txt /input/
  - 查找hadoop安装目录下的示例程序
    find /usr/local/hadoop/ -name *example*.jar
    /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar
  - 执行MapReduce示例WordCount，统计/input/input.txt中的单词数量
    hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount /input/input.txt /output
  - 查看控制台输出的JobID
  - 通过网页查询Yarn中Application的状态，以及任务执行状态
  - 执行完成后在HDFS的/output目录查看执行结果
    任务执行成功：_SUCCESS
    任务执行结果：part-r-00000
    hadoop fs -text /output/part-r-00000
      c         1
      c#        1
      c++       1
      hello     10
      java      3
      kotlin    1
      perl      1
      python    1
      scala     1

================================

# 计算引擎Map/Reduce

1.WordCount工作过程
  input  : 输入，input.txt的内容
  split  : 拆分，默认按行拆分，形成K-V对（起始位置->数据行）
  map    : 映射，计算split结果中的数据行，形成K-V对（单词->1，可能存在的缩减：单词->n）
  shuffle: 派发，将map的结果分发到各结点进行计算（单词->[1,1,n,1,...]）
  reduce : 缩减，对shuffle过程分发过来的数据进行缩减计算得到结果（单词->数量）
  output : 输出，最终结果

2.Java开发MpaReduce程序

  配置环境变量
  - HADOOP_HOME=hadoop的安装目录(路径不要包含空格或中文)
  - 为方便操作，将%HADOOP_HOME%\bin添加到Path环境变量

  对于windows系统，需要添加windows的库文件
  - 将Windows库文件覆盖到%HADOOP_HOME%\bin
  - 若有问题，可将hadoop.dll复制到C:\Windows\System32目录
  设置系统环境变量HADOOP_USER_NAME=root

  Map代码：src/main/java/com/my/project/WordMapper.java
  Reduce代码：src/main/java/com/my/project/WordReducer.java
  MapReduce测试代码：src/test/java/com/my/project/TestMapReduce.java

  在远程Yarn上执行MapReduce程序时除了使用conf.setXXX设置相关属性
    conf.set("fs.defaultFS", "hdfs://192.168.56.10:9000/");
    conf.set("mapreduce.job.jar", "target/hello-hadoop-0.0.1.jar");
    conf.set("mapreduce.framework.name", "yarn");
    conf.set("yarn.resourcemanager.hostname", "192.168.56.10");
    conf.set("mapreduce.app-submission.cross-platform", "true");
  也可以将hadoop的四个配置文件加到src/main/resources目录
    core-site.xml
    hdfs-site.xml
    mapred-site.xml
    yarn-site.xml

================================

# Hive入门

1.下载上传apache-hive-2.1.1-bin.tar.gz到master，解压到/usr/local/

2.配置环境变量HADOOP_HOME和HIVE_HOME
  vim /etc/profile.d/hadoop.sh
    export HADOOP_HOME=/usr/local/hadoop
    export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
  vim /etc/profile.d/hive.sh
    export HIVE_HOME=/usr/local/hive
    export PATH=${PATH}:${HIVE_HOME}/bin
  重启master虚拟机

3.修改配置文件hive-site.xml
  cp hive-default.xml.template hive-site.xml
  修改hive.metastore.schema.verification=false
  mkdir /usr/local/hive/tmp
  替换${system:java.io.tmpdir}为/usr/local/hive/tmp
  替换${system:user.name}为root

4.初始化元数据的数据库
  schematool -initSchema -dbType derby
  以上命令会在当前目录下建立metastore_db的数据库
  注意：下一次启动hive时应该还在同一目录，默认到当前目录下寻找metastore
  遇到问题，把metastore_db删除，重新执行命令即可
  实际工作环境中，经常使用mysql作为metastore的数据

5.先启动HDFS，再启动hive：hive
  show databases;
  use default;
  show tables;
  create table doc(line string);
  desc doc;
  select * from doc;
  drop table doc;
  exit;

  注意：
  - 启动中日志输出：Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions.
    Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
  - 可以在hive中执行命令查看HDFS上的文件：dfs -ls /user/hive/warehouse/doc/;

6.启动Yarn，执行示例程序
  load data inpath '/input/' overwrite into table doc; //注意这里并没有copy数据，只是将数据和表做了关联
  select * from doc;
  select split(line, ' ') from doc;
  select explode(split(line, ' ')) from doc;
  select word, count(1) as count from (select explode(split(line, ' ')) as word from doc) w group by word;
  create table word_counts as select word, count(1) as count from (select explode(split(line, ' ')) as word from doc) w group by word;
  select * from word_counts;
  dfs -ls /user/hive/warehouse/word_counts/

7.使用Sougou搜索日志做实验，统计热搜榜
  将日志文件上传到HDFS，启动hive
  create table sougou(qtime string, qid string, qword string, url string) row format delimited fields terminated by ',';
  load data inpath '/sougou.dic' into table sougou; //注意这里并没有copy数据，只是将数据和表做了关联
  select count(*) from sougou;
  create table sougou_results as select keyword, count(1) as count from (select qword as keyword from sougou) t group by keyword order by count desc;
  select * from sougou_results limit 10;

8.HiveQL的翻译引擎，用类似SQL语言的方式做计算
  将SQL语句查询，转换为MapReduce操作，省去Java编程
  应用场合：静态数据分析，数据不会频繁变化，不需要实时结果响应
  不支持行级的插入/删除操作，不支持事务
  注：测试中发现load data时源文件会被移动到/user/hive/warehouse/[table_name]/下，但实际数据块并没有发生移动，如果drop了表则数据文件也会被一并删除

================================

# Spark入门

1.内存计算引擎，使用Scala开发，支持Java/Scala/Python开发接口

2.运行模式

  local模式

  - 使用spark-submit提交job
      cd /usr/local/spark
      ./bin/spark-submit --class org.apache.spark.examples.SparkPi ./examples/jars/spark-examples_2.11-2.2.0.jar 10000
    通过http://192.168.56.14:4040/查看job执行情况

  - 使用spark-shell进行交互式提交（spark-shell不加参数默认进入local模式）
      ./bin/spark-shell
    再次连接一个terminal，用jps观察进程，会看到spark-submit进程
    获取命令帮助
      :help
    退出spark-shell
      :quit

  - 使用Scala编程的方式执行WordCount，可通过http://192.168.56.14:4040/观察命令执行情况
      sc
      val lineRDD = sc.textFile("/root/hello.txt")
      lineRDD.foreach(println)
      lineRDD.collect
      val wordRDD = lineRDD.flatMap(line => line.split(" "))
      wordRDD.collect
      val wordCountRDD = wordRDD.map(word => (word, 1))
      wordCountRDD.collect
      val resultRDD = wordCountRDD.reduceByKey((x, y) => x + y)
      resultRDD.collect
      var orderedRDD = resultRDD.sortByKey()
      orderedRDD.collect
    将执行结果写到/root/result/目录
      orderedRDD.saveAsTextFile("/root/result")
    简单写法
      sc.textFile("/root/hello.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).sortByKey().collect

  - 使用local模式访问HDFS数据
    启动HDFS：start-dfs.sh
    在spark-shell中执行以下命令：
      sc.textFile("hdfs://192.168.56.10:9000/input/input.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).sortByKey().saveAsTextFile("hdfs://192.168.56.10:9000/output3")
    在HDFS上查看执行结果：hadoop fs -cat /output3/part-00000

  standalone模式
  
  - 在master和所有slave上解压spark
  - 修改master上conf/slaves文件，加入slave主机名
  - 修改conf/spark-env.sh，export SPARK_MASTER_HOST=master
  - 将spark-env.sh复制到每一台slave
  - cd /usr/local/spark
  - ./sbin/start-all.sh
  - 在spark测试主机上执行：./bin/spark-shell --master spark://192.168.56.10:7077（也可以使用配置文件）
      sc.textFile("hdfs://192.168.56.10:9000/input/input.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).sortByKey().saveAsTextFile("hdfs://192.168.56.10:9000/output4")
    在HDFS上查看执行结果
      hadoop fs -ls /output4
  - 观察http://192.168.56.10:8080/

  yarn模式

  - TODO Learn spark on yarn

  mesos模式

  - TODO Learn spark on mesos

3.核心概念RDD（弹性分布式数据集）

  通过spark-shell认识RDD(Resillient Distributed Dataset)

